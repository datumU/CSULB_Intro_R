---
title: "Linear Regression"
subtitle: CSULB Intro to R
date: April 27, 2018
#author: "Chris Galbraith"
output: slidy_presentation
smaller: yes
---

```{r, include=FALSE, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Agenda

1. Statistical Distributions (very brief) 

2. T-Tests

3. Linear Regression

    + Fitting Models
    + Interpretation
    + Comparing Models & Variable Selection
    + Diagnostics
    + Prediction


## The Normal Distribution

+ Very common distribution 

+ Sometimes called the *bell curve* due to its shape

+ Two paramaters: mean ($\mu$) and standard deviation ($\sigma$)

+ Many real-life applications:
    
    + Male heights: $\mu = 70\text{ in.}, \sigma = 4\text{ in.}$
    + Female heights: $\mu = 65\text{ in.}, \sigma = 3.5\text{ in.}$
    + Blood pressure: $\mu = 85\text{ mm}, \sigma = 20\text{ mm}$

![](normal_distn.png)


## Statistical Distributions in R:

+ R has many built-in statistical distributions
    + e.g., binomial, poisson, normal, chi square, ...

+ Each distribution in R has four functions:
    + These functions begin with a "d", "p", "q", or "r" and are followed by the name of the distribution

+ ```d<dist>()```: evaluates the probability density/mass function at a given value
+ ```r<dist>()```: generates random numbers
+ ```p<dist>()```: returns the cumulative distribution function (CDF) for a given quantile
+ ```q<dist>()```: returns the quantile for a given probability


## Standard Normal Distribution

Calculate the value of the probability density function at $X = 0$
```{r echo=TRUE}  
str(dnorm) # normal pdf
dnorm(x = 0, mean = 0, sd = 1)
```


## Standard Normal Distribution

Plot the density function using ```dnorm()```
```{r echo=TRUE, fig.height = 4.5, fig.align='center'}  
x <- seq(from = -3, to = 3, by = 0.05)
y <- dnorm(x, mean = 0, sd = 1)
plot(x, y, type = "l", ylab = "Density", lwd = 2)
```


## Standard Normal Distribution

Generate 10 independent random numbers from a standard normal distribution
```{r echo=TRUE}  
str(rnorm) # generate random number from normal dist
rnorm(10, mean = 0, sd = 1)
```


## Standard Normal Distribution

Calculate the probability that $X \leq 0$
```{r echo=TRUE}  
str(pnorm) # normal CDF
pnorm(0, mean = 0, sd = 1) # Pr[X <= 0] = ?
```


## Standard Normal Distribution 


```{r echo=TRUE}
str(qnorm)
qnorm(0.975)  # Pr[X <= ?] = 0.975
```

```{r echo=FALSE,fig.height = 4.5, fig.align='center'}
x <- seq(from = -3.5, to = 3.5, by = 0.01)
y <- dnorm(x, mean = 0, sd = 1)
plot(x, y, type = "l", ylab = "Density", lwd = 2, xlab="")
abline(v = qnorm(0.975), col="red", lty = 2, lwd = 2)
abline(v = qnorm(0.025), col="red", lty = 2, lwd = 2)
polygon( c( x[x>=qnorm(0.975)], qnorm(0.975) ),  c( y[x>=qnorm(0.975)], 0 ), col="red")
polygon( c( x[x<=qnorm(0.025)], qnorm(0.025) ),  c( y[x<=qnorm(0.025)], 0 ), col="red")
```



## T-Tests
T-tests can be used to draw statistical conclusions about parameters of interest in the data

+ Is the mean of this data different from zero (or another number)?
+ **Is this regression coefficient different from zero?**
+ Are the means of two data sets different from one another?

T-tests can be categorized into two groups:

1. **One-sample t-test**

2. Two-sample t-test


##  One-Sample T-Test (Create Data)
```{r echo=TRUE}
set.seed(123)
oneSampData <- rnorm(100, mean = 0, sd = 1)
mean(oneSampData)
sd(oneSampData)
```


##  One-Sample T-Test ($H_0: \mu = 0$)
```{r echo=TRUE}
oneSampTest.0 <- t.test(oneSampData) 
oneSampTest.0
```

```{r echo=TRUE}
names(oneSampTest.0) 
```  


## One-Sample T-Test ($H_0: \mu = 0$)

```{r echo=TRUE}
oneSampTest.0$statistic
oneSampTest.0$p.value
```  

```{r echo = FALSE, fig.height = 4.5, fig.align='center'}
x <- seq(from = -3.5, to = 3.5, by = 0.01)
y <- dt(x, df = 99)
plot(x, y, type = "l", ylab = "Density", lwd = 2, xlab="")
abline(v = oneSampTest.0$statistic, col="red", lty = 2, lwd = 2)
abline(v = -oneSampTest.0$statistic, col="red", lty = 2, lwd = 2)
polygon( c( x[x>=oneSampTest.0$statistic], oneSampTest.0$statistic ),  
         c( y[x>=oneSampTest.0$statistic], 0 ), col="red")
polygon( c( x[x<=-oneSampTest.0$statistic], -oneSampTest.0$statistic ),  
         c( y[x<=-oneSampTest.0$statistic], 0 ), col="red")
```


## One-Sample T-Test ($H_0: \mu = 0.3$)

```{r echo=TRUE}
oneSampTest.mu <- t.test(oneSampData, mu = 0.3)
oneSampTest.mu
```  


```{r echo = FALSE, fig.height = 4.5, fig.align='center'}
x <- seq(from = -3.5, to = 3.5, by = 0.01)
y <- dt(x, df = 99)
plot(x, y, type = "l", ylab = "Density", lwd = 2, xlab="")
abline(v = oneSampTest.mu$statistic, col="red", lty = 2, lwd = 2)
abline(v = -oneSampTest.mu$statistic, col="red", lty = 2, lwd = 2)
polygon( c( x[x<=oneSampTest.mu$statistic], oneSampTest.mu$statistic ),  
         c( y[x<=oneSampTest.mu$statistic], 0 ), col="red")
polygon( c( x[x>=-oneSampTest.mu$statistic], -oneSampTest.mu$statistic ),  
         c( y[x>=-oneSampTest.mu$statistic], 0 ), col="red")
```


## Linear Regression
+ Approach to model the relationship between a *response* variable $Y$ and one or more *predictor* variables $X$.

+ $Y$ is a noisy linear combination of $X$: $$Y = X \beta + \epsilon$$

    + $Y$ is a $n \times 1$ vector of response variables
    + $X$ is a $n \times p$ matrix where each row $X_i$ is the predictor variables for the $i^{th}$ observation
    + $\beta$ is the $p \times 1$ vector of regression coefficients
    + $\epsilon \sim N(0,\sigma^2 I)$ is the $n \times 1$ error vector 
    + Implies that $Y \sim N(X\beta, \sigma^2I)$

+ Linear regression equivalent to modeling the mean of $Y$. Namely, $\mu = E(Y) = X\beta$

+ Multiple ways to estimate $\beta$, but we will focus on the most basic method which is **ordinary least squares estimation**.

    + Minimize sum of squared error (SSE): $\sum_{i=1}^{n} (y_i - X_i\beta)^2$
    + Closed form solution: $\hat{\beta} = (X^T X)^{-1} X^T Y$
    + Unbiased and consistent


## Linear Regression in R
+ We use ```lm()``` to fit linear regression models, which has requires us to pass in a regression formula.

+ Don't need to create the matrix $X$ (R does this for us), just tell it which predictors to include.

+ Some notes on ```formula```:

    + Response variable goes on the left and the predictors go on the right of a ```~```
    + Predictors are separated by ```+``` symbols (e.g., ```y ~ x + z```)
    + If you want to use all variables in a data frame, simply use ```.``` (e.g., ```y ~ .```)
    + By default, the formula contains an intercept. If you want to exclude it (i.e., force the intercept to be zero), can use either of ```y ~ 0 + .``` or ```y ~ . - 1```
    + Can include polynomial terms: Say we want to have a quadratic regression of ```y``` on ```x```, we simply use ```y ~ x + I(x^2)```


## Linear Regression - Prestige Data
We will fit a linear model using the prestige data. Recall that we have the following variables

+ ```education```: Average education of occupational incumbents, years, in 1971.

+ ```income```: Average income of incumbents, dollars, in 1971.

+ ```women```: Percentage of incumbents who are women.

+ ```prestige```: Pineo-Porter prestige score for occupation, from a social survey conducted in the mid-1960s.

+ ```census```: Canadian Census occupational code.

+ ```type```: Type of occupation, a factor with levels

    + ```bc```: Blue Collar
    + ```prof```: Professional, Managerial, and Technical
    + ```wc```: White Collar


## Load Data
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
prestige <- read.csv(file = here::here("data", "prestige_v2.csv"), 
                     row.names=1)
str(prestige)
head(prestige)
```


## Another look at the scatterplot matrix
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
library(car)
scatterplotMatrix(prestige[,c("prestige","education","income","women")])
```


## Linear Regression - Fit the Model
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
myReg <- lm(prestige ~ education + income + women, data = prestige)
myReg
names(myReg)
```


##  Linear Regression - Summary of Fit
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
summary(myReg)
```


##  Linear Regression - Summary Contents
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
myReg.summary <- summary(myReg)
names(myReg.summary) # show different contents
names(myReg) # this is what we had previously
```


##  Linear Regression - Confidence Intervals
+ 95% confidence interval for coefficient of ```income```
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
confint(myReg, 'income', level=0.95)
```

+ 95% confidence interval for each coefficient
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
confint(myReg, level=0.95)
```


##  Linear Regression - Adding Variables
Recall that ```type``` of occupation has a relationship with prestige score.
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, fig.height=5*(3/4), fig.width=5}
boxplot(prestige ~ type, data = Prestige, col = "grey",
        main = "Distribution of Prestige Score by Types",
        xlab = "Occupation Types", ylab = "Prestige Score")
```

So let's add the predictor ```type``` into our regression model:
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
mod <- update(myReg, ~ . + type)
summary(mod)
```


##  Linear Regression - Comparing Models
Suppose we want to test which of our two models is better when one model is *nested* within the other (i.e., both models contain the same terms and one has at least one additional term). 
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
formula(myReg)
formula(mod)
```

We say that ```myReg``` is *nested within* ```mod```, or equivantley that ```myReg``` is the **reduced** model and ```mod``` is the **full** model.

Hypothesis Test:

+ Null: Reduced model fits as well as the full model (formally, coefficients for the omitted terms are all zero).
    
+ Alternative: Full model fits better than the reduced model (formally, at least one omitted coefficient is non-zero).


##  Linear Regression - Comparing Models, contd.
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
anova(myReg, mod)
```
So we reject the null hypothesis and conclude that at least one of the levels of type of occupation contributes information about the prestige score. 


##  Linear Regression - Relevel a Factor
Say we want the change the reference level for the covariate ```type``` so that the intercept of the model includes is for professionals. We simply relevel the variable ```type``` and re-fit the model. 
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
levels(prestige$type)
prestige$type <- relevel(prestige$type, "prof")
levels(prestige$type)
```

**Note: This does not change the results of the model (e.g., predictions, F-statistics) but does change the interpretation and p-values of the type coefficients.**

##  Linear Regression - Relevel a Factor, contd.
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
mod <- update(myReg, ~ . + type)
summary(mod)
```


## Linear Regression - More on Variable Selection
+ Automated algorithms for automatic variable selection exist, but should be used with caution. 

+ Better to start with an *a priori* set of variables you need to include and then compare models, especially when goal is assocation instead of prediction.

+ **Stepwise regression**: start with a model and add (or remove) predictors until some criteria is met


## Stepwise Regression

```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.width=8}
null <- lm(prestige ~ 1, data=prestige)  # most basic model, intercept only
full <- lm(prestige ~ education + income + women + type, data=prestige)  # saturated model, all predictors
step(null, scope=list(lower=null, upper=full), direction='forward')
step(full, scope=list(lower=null, upper=full), direction='backward')
step(null, scope=list(lower=null, upper=full), direction='both')
```


##  Linear Regression - Diagnostics
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.width=8}
par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))
plot(myReg)
```


##  Linear Regression - Prediction
Predict the output for a new input
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
newData = data.frame(education=13.2, income=12000, women=12)
predict(myReg, newData, interval="predict")
```


## End of Session 2
Next up:

1. [Exercise 2](https://datumu.github.io/CSULB_Intro_R/day_2/exercises/exercise_2/ex_2.html)
2. Break

**Return at 3:00 to discuss solutions to Exercise 2!**
